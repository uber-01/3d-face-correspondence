{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense 3D Face Correspondence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T15:13:40.534936Z",
     "start_time": "2019-05-12T15:13:40.531847Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\" \n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\" \n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T15:33:20.683706Z",
     "start_time": "2019-05-12T15:33:20.680327Z"
    }
   },
   "outputs": [],
   "source": [
    "import pdb\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import threading\n",
    "import warnings\n",
    "import cv2\n",
    "import ipyvolume as ipv\n",
    "import scipy\n",
    "from math import cos, sin\n",
    "from scipy import meshgrid, interpolate\n",
    "import pdb\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.spatial import ConvexHull, Delaunay\n",
    "import numpy as np\n",
    "from scipy.interpolate import griddata\n",
    "from collections import defaultdict\n",
    "\n",
    "# THRESHOLDS\n",
    "rho = 0.5\n",
    "eigen_ratio_threshold = 5000\n",
    "Kq = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read each face data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T15:13:40.878273Z",
     "start_time": "2019-05-12T15:13:40.874955Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_wrl(file_path):\n",
    "    holder = []\n",
    "    with open(file_path, \"r\") as vrml:\n",
    "        for line in vrml:\n",
    "            a = line.strip().strip(\",\").split()\n",
    "            if len(a) == 3:\n",
    "                try:\n",
    "                    holder.append(list(map(float, a)))\n",
    "                except:\n",
    "                    pass\n",
    "    x,y,z = zip(*holder)\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    z = np.array(z)\n",
    "    return np.array(holder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T15:13:41.225816Z",
     "start_time": "2019-05-12T15:13:40.879204Z"
    }
   },
   "outputs": [],
   "source": [
    "file_paths = {\n",
    "    \"path1\": \"F0001/F0001_AN01WH_F3D.wrl\",\n",
    "    \"path2\": \"F0001/F0001_AN02WH_F3D.wrl\",\n",
    "    \"path3\": \"F0001/F0001_AN03WH_F3D.wrl\",\n",
    "    \"path4\": \"F0001/F0001_AN04WH_F3D.wrl\",\n",
    "    \"path5\": \"F0001/F0001_DI01WH_F3D.wrl\",\n",
    "    \"path6\": \"F0001/F0001_DI02WH_F3D.wrl\",\n",
    "    \"path7\": \"F0001/F0001_DI03WH_F3D.wrl\",\n",
    "    \"path8\": \"F0001/F0001_DI04WH_F3D.wrl\",\n",
    "    \"path9\": \"F0001/F0001_FE01WH_F3D.wrl\",\n",
    "    \"path10\": \"F0001/F0001_FE02WH_F3D.wrl\",\n",
    "    \"path11\": \"F0001/F0001_FE03WH_F3D.wrl\",\n",
    "    \"path12\": \"F0001/F0001_FE04WH_F3D.wrl\",\n",
    "    \n",
    "}\n",
    "face_points = {} # key = face+index, value = extracted face data\n",
    "for i in range(1, len(file_paths)+1):\n",
    "    face_points[\"face\" + str(i)] = read_wrl(file_paths[\"path\" + str(i)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing faces and Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T15:13:41.230918Z",
     "start_time": "2019-05-12T15:13:41.227251Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def normalize_face(points):\n",
    "    maxind = np.argmax(points[:,2])\n",
    "    nosex = points[maxind,0]\n",
    "    nosey = points[maxind,1]\n",
    "    nosez = points[maxind,2]\n",
    "    points = points - np.array([nosex, nosey, nosez])\n",
    "#     points = points / np.max(points)\n",
    "    return points\n",
    "\n",
    "def points2grid(points):\n",
    "    x1, y1, z1 = map(np.array, zip(*points))\n",
    "    grid_x, grid_y = np.mgrid[np.amin(x1):np.amax(x1):0.5, np.amin(y1):np.amax(y1):0.5]\n",
    "    grid_z = griddata((x1, y1), z1, (grid_x, grid_y), method='linear')\n",
    "    return [grid_x, grid_y, grid_z]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T15:13:42.117217Z",
     "start_time": "2019-05-12T15:13:41.232286Z"
    }
   },
   "outputs": [],
   "source": [
    "# normalizing the faces and interpolating them across a grid\n",
    "grid_data = {}\n",
    "for i in range(1, len(file_paths)+1):\n",
    "    # normalization\n",
    "    face_points[\"face\" + str(i)] = normalize_face(face_points[\"face\" + str(i)])\n",
    "    # grid interpolation of the face data\n",
    "    grid_data[\"face\" + str(i)] = points2grid(face_points[\"face\" + str(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T15:13:42.148643Z",
     "start_time": "2019-05-12T15:13:42.118175Z"
    }
   },
   "outputs": [],
   "source": [
    "# hull plot\n",
    "first_face_id = 3 # face id (number) to plot\n",
    "second_face_id = 7\n",
    "points = face_points[\"face\" + str(first_face_id)]\n",
    "points2 = face_points[\"face\" + str(second_face_id)]\n",
    "ipv.quickscatter(points[:, 0], points[:, 1], points[:, 2], size=1, marker=\"sphere\")\n",
    "ipv.scatter(points2[:, 0], points2[:, 1], points2[:, 2], size=1, marker=\"sphere\", color=\"blue\")\n",
    "ipv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the interpolated faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T15:13:42.187696Z",
     "start_time": "2019-05-12T15:13:42.149712Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "face_id = 8\n",
    "ipv.clear()\n",
    "grid_x, grid_y, grid_z = grid_data[\"face\" + str(face_id)]\n",
    "ipv.plot_mesh(grid_x, grid_y, grid_z, wireframe=False)\n",
    "ipv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Correspondence Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seed points sampling using mean 2D convex hull "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T15:13:48.385815Z",
     "start_time": "2019-05-12T15:13:42.189114Z"
    }
   },
   "outputs": [],
   "source": [
    "def hull72(points, nosex, nosey, nosez):\n",
    "    newhull = [[nosex, nosey, nosez]]\n",
    "    for theta in range(0, 360, 5):\n",
    "        fx = 200 * cos(theta * np.pi / 180)\n",
    "        fy = 200 * sin(theta * np.pi / 180)\n",
    "        nearest_point = min(zip(points[:, 0], points[:, 1], points[:, 2]), key=lambda p:(p[0] - fx)**2 + (p[1] - fy)**2)\n",
    "        newhull.append(nearest_point)\n",
    "    return newhull\n",
    "\n",
    "def get_hull(points):\n",
    "    maxind = np.argmax(points[:,2])\n",
    "    # coordinates of nose, nosex = x coordinate of nose, similarly for nosey and nosez\n",
    "    nosex = points[maxind,0]\n",
    "    nosey = points[maxind,1]\n",
    "    nosez = points[maxind,2]\n",
    "    hull = np.array(hull72(points, nosex,nosey,nosez))\n",
    "    return hull\n",
    "\n",
    "hull = np.zeros([73, 3])\n",
    "for i in range(1, len(file_paths)+1):\n",
    "    hull += get_hull(face_points[\"face\" + str(i)])\n",
    "hull = hull / len(file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of extracted seed points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T15:13:48.616700Z",
     "start_time": "2019-05-12T15:13:48.386715Z"
    }
   },
   "outputs": [],
   "source": [
    "first_face_index = 7\n",
    "second_face_index = 4\n",
    "points1 = face_points[\"face\" + str(first_face_index)]\n",
    "points2 = face_points[\"face\" + str(second_face_index)]\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(points1[:,0], points1[:,1], color=\"red\")\n",
    "plt.scatter(points2[:,0], points2[:,1], color=\"orange\")\n",
    "plt.scatter(hull[:,0], hull[:,1])\n",
    "maxind = np.argmax(points1[:,2])\n",
    "nosex = points1[maxind,0]\n",
    "nosey = points1[maxind,1]\n",
    "nosez = points1[maxind,2]\n",
    "plt.plot(nosex,nosey,\"b\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delaunay Triangulation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T15:13:48.619753Z",
     "start_time": "2019-05-12T15:13:48.617584Z"
    }
   },
   "outputs": [],
   "source": [
    "def triangulation(hull):\n",
    "    points2D = np.vstack([hull[:,0],hull[:,1]]).T\n",
    "    tri_hull = Delaunay(points2D) \n",
    "    return tri_hull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T15:13:48.630582Z",
     "start_time": "2019-05-12T15:13:48.620648Z"
    }
   },
   "outputs": [],
   "source": [
    "tri_hull = triangulation(hull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T15:13:48.666916Z",
     "start_time": "2019-05-12T15:13:48.631710Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#tri_hull = triangulation(hull)\n",
    "ipv.figure()\n",
    "mesh = ipv.plot_trisurf(hull[:,0], hull[:,1], hull[:,2], triangles=tri_hull.simplices, color='blue')\n",
    "ipv.scatter(hull[:,0], hull[:,1], hull[:,2], marker='sphere', color='red')\n",
    "ipv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geodesic Patch Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T15:13:48.677260Z",
     "start_time": "2019-05-12T15:13:48.668188Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_all_patches_from_face(points, hull, triangles):\n",
    "    from itertools import combinations\n",
    "    \n",
    "    patch_width = 5 * rho\n",
    "    def distance(x,y,z,x1,y1,z1,x2,y2,z2):\n",
    "        a = (y2-y1)/(x2-x1)\n",
    "        b = -1\n",
    "        c = y2-x2*(y2-y1)/(x2-x1)\n",
    "        return abs(a*x+b*y+c)/(a**2+b**2)**0.5\n",
    "    \n",
    "    patches = []\n",
    "    for t1,t2 in combinations(triangles,r=2): #pairwise triangles\n",
    "        if len(set(t1)&set(t2))==2:           #triangles with a common edge\n",
    "            patch = []\n",
    "            a_ind, b_ind = list(set(t1)&set(t2))\n",
    "            x1, y1, z1 = hull[a_ind,:]\n",
    "            x2, y2, z2 = hull[b_ind,:]\n",
    "            for x,y,z in points: #loop over all points to find patch points\n",
    "                if (x-x1/2-x2/2)**2+(y-y1/2-y2/2)**2<(x1/2-x2/2)**2+(y1/2-y2/2)**2 and distance(x,y,z,x1,y1,z1,x2,y2,z2)<patch_width:\n",
    "                    patch.append([x,y,z])\n",
    "            if len(patch)==0: \n",
    "                #print(\"ALERT: NO PATCH FOR AN EDGE!!!!\")\n",
    "                pass\n",
    "            patches.append(np.array(patch))\n",
    "    return patches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T15:14:17.067404Z",
     "start_time": "2019-05-12T15:13:48.678547Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_patches(hull, triangles):\n",
    "    #pdb.set_trace()\n",
    "    patches = defaultdict(list) # key = edges, values = a list of extracted patches from all faces along that edge \n",
    "    for face_index in range(1, len(file_paths)+1):\n",
    "        all_patches = get_all_patches_from_face(face_points[\"face\"+str(face_index)], hull, triangles)\n",
    "        #print(len(all_patches))\n",
    "        # the patches are organised in following way because the original get_patches function was modified after the whole serial code was written\n",
    "        try:\n",
    "            for edge_index in range(len(all_patches)):\n",
    "                patches[\"edge\" + str(edge_index)].append(all_patches[edge_index])\n",
    "        except:\n",
    "            pdb.set_trace()\n",
    "    return patches\n",
    "#hull= correspondence_set\n",
    "patches = get_patches(hull, tri_hull.simplices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T15:14:17.167067Z",
     "start_time": "2019-05-12T15:14:17.068476Z"
    }
   },
   "outputs": [],
   "source": [
    "# randomly selecting a edge, and plotting all patches along that edge across all the faces\n",
    "ipv.clear()\n",
    "edge_index = np.random.choice(range(len(patches)))\n",
    "edge = patches[\"edge\" + str(edge_index)]\n",
    "for i in range(len(edge)):\n",
    "    patch = edge[i]\n",
    "    ipv.scatter(patch[:,0], patch[:,1], patch[:,2], size=1, marker=\"sphere\", color=[\"red\", \"blue\", \"yellow\", \"green\"][i%4])\n",
    "ipv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keypoint Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T15:14:17.172739Z",
     "start_time": "2019-05-12T15:14:17.169037Z"
    }
   },
   "outputs": [],
   "source": [
    "# takes in a point and the patch it belongs to and decides whether it is a keypoint (ratio of largest two eigenvalues on the covariance matrix of its local surface) or not\n",
    "def is_keypoint(point, points):\n",
    "    threshold = 7 * rho\n",
    "    nhood = points[(np.sum(np.square(points-point),axis=1)) < threshold**2]\n",
    "    try:\n",
    "        nhood = (nhood - np.min(nhood, axis=0)) / (np.max(nhood, axis=0) - np.min(nhood, axis=0))\n",
    "        covmat = np.cov(nhood)\n",
    "        eigvals = np.sort(np.abs(np.linalg.eigvalsh(covmat)))\n",
    "        ratio = eigvals[-1]/(eigvals[-2]+0.0001)\n",
    "        return ratio>30 #eigen_ratio_threshold #/ 5\n",
    "    except Exception as e:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T15:14:25.487729Z",
     "start_time": "2019-05-12T15:14:17.173779Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_keypoints(patches):\n",
    "    keypoints = {} # key = edge, value = a list of keypoints extracted from the patches along that edge across all faces\n",
    "    for edge_index in range(1, len(patches)+1):\n",
    "        edge_patches = patches[\"edge\" + str(edge_index)]\n",
    "        edge_keypoints = []\n",
    "        for patch in edge_patches:\n",
    "            #print(patch.shape)\n",
    "            if patch.shape[0]:\n",
    "                patch_keypoints = patch[np.apply_along_axis(is_keypoint, 1, patch, patch)] # keypoints in `patch`\n",
    "            else:\n",
    "                patch_keypoints = []\n",
    "            edge_keypoints.append(patch_keypoints)\n",
    "        keypoints[\"edge\" + str(edge_index)] = edge_keypoints \n",
    "    return keypoints\n",
    "\n",
    "keypoints = get_keypoints(patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T15:14:25.905212Z",
     "start_time": "2019-05-12T15:14:25.873465Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot keypoints of a face, given the face index\n",
    "face_index = 1\n",
    "face_keypoints = []\n",
    "for edge_index in range(1, len(keypoints)+1):\n",
    "    try:\n",
    "        face_keypoints.extend(keypoints[\"edge\" + str(edge_index)][face_index-1])\n",
    "    except: # not every edge has a patch and hence keypoints, indexing an empty array will give error \n",
    "        pass\n",
    "face_keypoints = np.array(face_keypoints)\n",
    "print(face_keypoints.shape)\n",
    "#print(face_keypoints)\n",
    "points = face_points[\"face\" + str(face_index)]\n",
    "ipv.clear()\n",
    "ipv.scatter(points[:,0], points[:,1], points[:,2], size=1, marker=\"sphere\", color=\"blue\")\n",
    "ipv.scatter(face_keypoints[:, 0], face_keypoints[:,1], face_keypoints[:,2], size=1, marker=\"sphere\", color=\"red\")\n",
    "ipv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T15:14:25.912234Z",
     "start_time": "2019-05-12T15:14:25.906277Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_normal(x, y, grid_x, grid_y, grid_z):\n",
    "    '''\n",
    "      3\n",
    "    1   2\n",
    "      4\n",
    "    x, y are coordinates of the point for which the normal has to be calculated\n",
    "    '''\n",
    "    i = (x - grid_x[0, 0]) / (grid_x[1, 0] - grid_x[0, 0])\n",
    "    j = (y - grid_y[0, 0]) / (grid_y[0, 1] - grid_y[0, 0])\n",
    "    i,j = int(round(i)), int(round(j))\n",
    "    if (not 0 <= i < grid_x.shape[0]-1) or (not 0 <= j < grid_y.shape[1]-1):\n",
    "        warnings.warn(\"out of bounds error\")\n",
    "        #pdb.set_trace()\n",
    "        return \"None\"\n",
    "    point1 = (grid_x[i-1, j], grid_y[i-1, j], grid_z[i-1, j])\n",
    "    point2 = (grid_x[i+1, j], grid_y[i+1, j], grid_z[i+1, j])\n",
    "    point3 = (grid_x[i, j-1], grid_y[i, j-1], grid_z[i, j-1])\n",
    "    point4 = (grid_x[i, j+1], grid_y[i, j+1], grid_z[i, j+1])\n",
    "    a1, a2, a3 = [point2[x] - point1[x] for x in range(3)]\n",
    "    b1, b2, b3 = [point3[x] - point4[x] for x in range(3)]\n",
    "    normal = np.array([a3*b2, a1*b3, -a1*b2])\n",
    "    return normal/np.linalg.norm(normal)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T15:15:20.947432Z",
     "start_time": "2019-05-12T15:15:20.903618Z"
    }
   },
   "outputs": [],
   "source": [
    "# test the get_normal function and plot \n",
    "'''If this snippet throws an error, rerun it'''\n",
    "def normal_plot():\n",
    "    face_id = 8\n",
    "    grid_x, grid_y, grid_z = grid_data[\"face\" + str(face_id)]\n",
    "    i = np.random.choice(len(grid_x))\n",
    "    j = np.random.choice(len(grid_y))\n",
    "    x, y = grid_x[i, 0], grid_y[0, j]\n",
    "    print(i, j, x, y)\n",
    "    uvn = get_normal(x, y, grid_x, grid_y, grid_z)\n",
    "\n",
    "    ipv.clear()\n",
    "    ipv.plot_mesh(grid_x, grid_y, grid_z, wireframe=False)\n",
    "    ipv.quiver(np.array([x, ]), np.array([y,]), np.array([grid_z[i, j]]), np.array([uvn[0]]), np.array([uvn[1]]), np.array([uvn[2]]), color=\"blue\", size=10)\n",
    "    ipv.show()\n",
    "\n",
    "try:\n",
    "    normal_plot()\n",
    "except:\n",
    "    normal_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T15:15:35.320148Z",
     "start_time": "2019-05-12T15:15:35.314614Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_keypoint_features(keypoints, face_index):\n",
    "    feature_list = [] # a list to store extracted features of each keypoint\n",
    "    final_keypoints = [] # remove unwanted keypoints, like the ones on edges etc\n",
    "    for point in keypoints:\n",
    "        point_features = []\n",
    "        x, y, z = point\n",
    "        points = face_points[\"face\" + str(face_index)]\n",
    "        grid_x, grid_y, grid_z = grid_data[\"face\" + str(face_index)]\n",
    "        threshold = 5 * rho\n",
    "        nhood = points[(np.sum(np.square(points-point), axis=1)) < threshold**2]\n",
    "        xy_hu_moments = cv2.HuMoments(cv2.moments(nhood[:, :2])).flatten()\n",
    "        yz_hu_moments = cv2.HuMoments(cv2.moments(nhood[:, 1:])).flatten()\n",
    "        xz_hu_moments = cv2.HuMoments(cv2.moments(nhood[:, ::2])).flatten()\n",
    "        hu_moments = np.concatenate([xy_hu_moments, yz_hu_moments, xz_hu_moments])\n",
    "        #print(hu_moments)\n",
    "        #i = (x - grid_x[0, 0]) / (grid_x[1, 0] - grid_x[0, 0])\n",
    "        #j = (y - grid_y[0, 0]) / (grid_y[0, 1] - grid_y[0, 0])\n",
    "        #i, j = int(round(i)), int(round(j))\n",
    "        #start_i, start_j = i - int(5 * rho / (grid_x[1, 0] - grid_x[0, 0])), j - int(5 * rho / (grid_y[0, 1] - grid_y[0, 0]))\n",
    "        #end_i, end_j = i + int(5 * rho / (grid_x[1, 0] - grid_x[0, 0])), j + int(5 * rho / (grid_y[0, 1] - grid_y[0, 0]))\n",
    "        #nhood = points[start_i: end_i, start_j: end_j] \n",
    "        #nhood_x = grid_x[start_i:end_i, start_j:end_j]\n",
    "        #nhood_y = grid_y[start_i:end_i, start_j:end_j]\n",
    "        #nhood_z = grid_z[start_i:end_i, start_j:end_j]\n",
    "        normal = get_normal(x, y, grid_x, grid_y, grid_z)\n",
    "        if normal == \"None\": # array comparision raises ambiguity error, so None passed as string\n",
    "            continue\n",
    "        final_keypoints.append(point)\n",
    "        point_features.extend(np.array([x, y, z])) # spatial location\n",
    "        point_features.extend(normal)\n",
    "        point_features.extend(hu_moments)\n",
    "        point_features = np.array(point_features)\n",
    "        \n",
    "        feature_list.append(point_features)\n",
    "    final_keypoints = np.array(final_keypoints)\n",
    "    return final_keypoints, feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T15:15:39.903585Z",
     "start_time": "2019-05-12T15:15:35.532602Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_features(keypoints):\n",
    "    features = {} # key = edge + edge_index, value = list of features for each keypoint across all the faces\n",
    "    for edge_index in range(1, len(keypoints)+1):\n",
    "        edgewise_keypoint_features = [] # store features of keypoints for a given edge_index across all faces\n",
    "        for face_index in range(1, len(file_paths)+1):\n",
    "            try:\n",
    "                edge_keypoints = keypoints[\"edge\" + str(edge_index)][face_index-1]\n",
    "                final_keypoints, keypoint_features = get_keypoint_features(edge_keypoints, face_index)\n",
    "                keypoints[\"edge\" + str(edge_index)][face_index-1] = final_keypoints # update the keypoint, remove unwanted keypoints like those on the edge etc\n",
    "            except: # for no keypoints, no features\n",
    "                keypoint_features = []\n",
    "            edgewise_keypoint_features.append(keypoint_features)\n",
    "        features[\"edge\" + str(edge_index)] = edgewise_keypoint_features\n",
    "    return features\n",
    "\n",
    "features = get_features(keypoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-10T11:46:51.337463Z",
     "start_time": "2019-05-10T11:46:51.334837Z"
    }
   },
   "source": [
    "## Keypoint matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T15:15:39.912030Z",
     "start_time": "2019-05-12T15:15:39.904859Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_keypoint_under_2rho(keypoints, point):\n",
    "    \"\"\"return the index of the keypoint in `keypoints` which is closest to `point` if that distance is less than 2 * rho, else return None\"\"\"\n",
    "    try:\n",
    "        distance = np.sqrt(np.sum(np.square(keypoints-point), axis=1))\n",
    "        if (distance < 4*rho).any():\n",
    "            min_dist_index = np.argmin(distance)\n",
    "            return min_dist_index\n",
    "    except Exception as e: # keypoints is [], gotta return None\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def get_matching_keypoints(edge_keypoints, edge_features, edge_index):\n",
    "    # check if a bunch of keypoints across the patches (across all faces) are withing 2*rho\n",
    "    # first get all the keypoints in a list\n",
    "    matching_keypoints_list = []\n",
    "    for face_index1 in range(len(edge_keypoints)): # take a patch along the edge among the faces\n",
    "        for point_index, point in enumerate(edge_keypoints[face_index1]): # take a keypoint in that patch, we have to find corresponding keypoints in each other patche along this edge\n",
    "            matched_keypoint_indices = [] # to store indices of matched keypoints across the patches\n",
    "            for face_index2 in range(len(edge_keypoints)): # find if matching keypoints exist across the patches along that edge across all faces\n",
    "                if face_index2 == face_index1: \n",
    "                    matched_keypoint_indices.append(point_index)\n",
    "                    continue\n",
    "                matched_keypoint = get_keypoint_under_2rho(edge_keypoints[face_index2], point)\n",
    "                if matched_keypoint:\n",
    "                    #if edge_index == 36: pdb.set_trace()I#\n",
    "                    matched_keypoint_indices.append(matched_keypoint)\n",
    "                else: # no keypoint was matched in the above patch (face_index2), gotta start search on other keypoint from face_index1\n",
    "                    break\n",
    "                        \n",
    "            if len(matched_keypoint_indices) == len(edge_keypoints): # there's a corresponding keypoint for each patch across all faces\n",
    "                 matching_keypoints_list.append(matched_keypoint_indices)\n",
    "    if len(matching_keypoints_list) == 0:\n",
    "        return []\n",
    "    # now we have those keypoints which are in vicinity of 2*rho, let's compute euclidean distance of their feature vectors\n",
    "    final_matched_keypoints = []\n",
    "    for matched_keypoints in matching_keypoints_list: # select first list of matching keypoints\n",
    "        # get the indices, get their corresponding features, compute euclidean distance\n",
    "        try:\n",
    "            features = np.array([edge_features[face_index][idx] for face_index, idx in zip(range(len(edge_features)), matched_keypoints)])\n",
    "            euc_dist_under_kq = lambda feature, features: np.sqrt(np.sum(np.square(features - feature), axis=1)) < Kq\n",
    "            if np.apply_along_axis(euc_dist_under_kq, 1, features, features).all() == True:\n",
    "                # we have got a set of matching keypoints, get their mean coordinates\n",
    "                matched_coords = [edge_keypoints[face_index][idx] for face_index, idx in zip(range(len(edge_features)), matched_keypoints)]\n",
    "                final_matched_keypoints.append(np.mean(matched_coords, axis=0))\n",
    "        except:\n",
    "            pdb.set_trace()\n",
    "    return final_matched_keypoints\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T15:15:40.328347Z",
     "start_time": "2019-05-12T15:15:39.913249Z"
    }
   },
   "outputs": [],
   "source": [
    "# those keypoints which are in vicinity of 2*rho are considered for matching\n",
    "# matching is done using constrained nearest neighbour\n",
    "# choose an edge, select a keypoint, find out keypoints on corresponding patches on other faces within a vicinity of 2*rho, \n",
    "# get euclidean distance in features among all possible pair wise combinations, if the distances come out to be less than Kp are added to the global set of correspondences\n",
    "def keypoint_matching_process(keypoints, features):\n",
    "    final_mean_keypoints = []\n",
    "    for edge_index in range(1, len(keypoints)):\n",
    "        edge_keypoints = keypoints[\"edge\" + str(edge_index)]\n",
    "        edge_features = features[\"edge\" + str(edge_index)]\n",
    "        matched_keypoints = get_matching_keypoints(edge_keypoints, edge_features, edge_index)\n",
    "        if len(matched_keypoints) == 0:\n",
    "            continue\n",
    "        #print(matched_keypoints)\n",
    "        final_mean_keypoints.extend(matched_keypoints)\n",
    "    #final_mean_keypoints = list(set(final_mean_keypoints))\n",
    "\n",
    "    final_mean_keypoints = np.array(final_mean_keypoints)\n",
    "    final_mean_keypoints = np.unique(final_mean_keypoints, axis=0)\n",
    "    return final_mean_keypoints\n",
    "\n",
    "final_mean_keypoints = keypoint_matching_process(keypoints, features)\n",
    "#print(\"Iteration completed\")\n",
    "#print(len(final_mean_keypoints), \"new keypoints found\")\n",
    "print(final_mean_keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T15:15:40.331740Z",
     "start_time": "2019-05-12T15:15:40.329862Z"
    }
   },
   "outputs": [],
   "source": [
    "updated_hull = np.concatenate((hull, final_mean_keypoints), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T15:15:40.561397Z",
     "start_time": "2019-05-12T15:15:40.332802Z"
    }
   },
   "outputs": [],
   "source": [
    "first_face_index = 7\n",
    "second_face_index = 4\n",
    "points1 = face_points[\"face\" + str(first_face_index)]\n",
    "points2 = face_points[\"face\" + str(second_face_index)]\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(points1[:,0], points1[:,1], color=\"red\")\n",
    "plt.scatter(points2[:,0], points2[:,1], color=\"orange\")\n",
    "#plt.scatter(updated_hull[:,0], updated_hull[:,1])\n",
    "\n",
    "plt.scatter(hull[:,0], hull[:,1])\n",
    "plt.scatter(final_mean_keypoints[:, 0], final_mean_keypoints[:, 1], color=\"yellow\")\n",
    "maxind = np.argmax(points1[:,2])\n",
    "nosex = points1[maxind,0]\n",
    "nosey = points1[maxind,1]\n",
    "nosez = points1[maxind,2]\n",
    "plt.plot(nosex,nosey,\"b\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T15:15:40.590825Z",
     "start_time": "2019-05-12T15:15:40.562581Z"
    }
   },
   "outputs": [],
   "source": [
    "ipv.clear()\n",
    "ipv.scatter(points[:,0], points[:,1], points[:,2], size=1, marker=\"sphere\", color=\"blue\")\n",
    "ipv.scatter(final_mean_keypoints[:, 0], final_mean_keypoints[:,1], final_mean_keypoints[:,2], size=2, marker=\"sphere\", color=\"red\")\n",
    "ipv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T15:15:40.624482Z",
     "start_time": "2019-05-12T15:15:40.591841Z"
    }
   },
   "outputs": [],
   "source": [
    "updated_tri_hull = triangulation(updated_hull)\n",
    "ipv.figure()\n",
    "mesh = ipv.plot_trisurf(updated_hull[:,0], updated_hull[:,1], updated_hull[:,2], triangles=updated_tri_hull.simplices, color='blue')\n",
    "ipv.scatter(updated_hull[:,0], updated_hull[:,1], updated_hull[:,2], marker='sphere', color='red')\n",
    "ipv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T15:19:21.583263Z",
     "start_time": "2019-05-12T15:15:40.625419Z"
    }
   },
   "outputs": [],
   "source": [
    "updated_patches = get_patches(updated_hull, updated_tri_hull.simplices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T15:19:21.586152Z",
     "start_time": "2019-05-12T15:19:21.584268Z"
    }
   },
   "outputs": [],
   "source": [
    "num_iterations = 10\n",
    "correspondence_set = hull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-12T15:33:28.117Z"
    }
   },
   "outputs": [],
   "source": [
    "# Start correspondence densification loop\n",
    "num_iterations = 10\n",
    "correspondence_set = hull\n",
    "for iteration in range(num_iterations):\n",
    "    print(\"\\n\\nStarting iteration: \", iteration)\n",
    "    t1 = time.time()\n",
    "    print(\"Starting Delaunay triangulation............\", end=\"\", flush=True)\n",
    "    tri_hull = triangulation(correspondence_set)\n",
    "    print(\"Done | time taken: %0.4f seconds\" % (time.time() - t1))\n",
    "\n",
    "    t2 = time.time()\n",
    "    print(\"Starting geodesic patch extraction............\", end=\"\", flush=True)\n",
    "    patches = get_patches(correspondence_set, tri_hull.simplices)\n",
    "    print(\"Done | time taken: %0.4f seconds\" % (time.time() - t2))\n",
    "\n",
    "    t3 = time.time()\n",
    "    print(\"Starting keypoint extraction............\", end=\"\", flush=True)\n",
    "    keypoints = get_keypoints(patches)\n",
    "    print(\"Done | time taken: %0.4f seconds\" % (time.time() - t3))\n",
    "\n",
    "    t4 = time.time()\n",
    "    print(\"Starting feature extraction............\", end=\"\", flush=True)\n",
    "    features = get_features(keypoints)\n",
    "    print(\"Done | time taken: %0.4f seconds\" % (time.time() - t4))\n",
    "\n",
    "    t5 = time.time()\n",
    "    print(\"Starting keypoint matching............\", end=\"\", flush=True)\n",
    "    final_mean_keypoints = keypoint_matching_process(keypoints, features)\n",
    "    print(\"Done | time taken: %0.4f seconds\" % (time.time() - t5))\n",
    "\n",
    "    print(\"Total new correspondences found: \", len(final_mean_keypoints))\n",
    "    print(\"Updating correspondence set...\")\n",
    "    correspondence_set = np.concatenate((correspondence_set, final_mean_keypoints), axis=0)\n",
    "    correspondence_set = np.unique(correspondence_set, axis=0)\n",
    "    print(\"Iteration completed in %0.4f seconds\" % (time.time() - t1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T15:14:26.014742Z",
     "start_time": "2019-05-12T15:13:40.641Z"
    }
   },
   "outputs": [],
   "source": [
    "len(correspondence_set)\n",
    "#correspondence_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T15:14:26.015300Z",
     "start_time": "2019-05-12T15:13:40.643Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "tri_hull = triangulation(correspondence_set)\n",
    "print(\"done\")\n",
    "print(\"starting geodesic patch extraction............\", end=\"\", flush=True)\n",
    "patches = get_patches(correspondence_set, tri_hull.simplices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T15:14:26.015845Z",
     "start_time": "2019-05-12T15:13:40.645Z"
    }
   },
   "outputs": [],
   "source": [
    "patches.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T15:14:26.016459Z",
     "start_time": "2019-05-12T15:13:40.648Z"
    }
   },
   "outputs": [],
   "source": [
    "ipv.clear()\n",
    "face_index = 3\n",
    "points = face_points[\"face\" + str(face_index)]\n",
    "ipv.scatter(points[:,0], points[:,1], points[:,2], size=1, marker=\"sphere\", color=\"blue\")\n",
    "for key in patches.keys():\n",
    "    patch = patches[key][face_index-1]\n",
    "    if len(patch):\n",
    "        ipv.scatter(patch[:, 0], patch[:,1], patch[:,2], size=2, marker=\"sphere\", color=\"red\")\n",
    "ipv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T15:14:26.017027Z",
     "start_time": "2019-05-12T15:13:40.649Z"
    }
   },
   "outputs": [],
   "source": [
    "updated_hull = correspondence_set\n",
    "updated_tri_hull = triangulation(updated_hull)\n",
    "ipv.figure()\n",
    "mesh = ipv.plot_trisurf(updated_hull[:,0], updated_hull[:,1], updated_hull[:,2], triangles=updated_tri_hull.simplices, color='blue')\n",
    "ipv.scatter(updated_hull[:,0], updated_hull[:,1], updated_hull[:,2], marker='sphere', color='red')\n",
    "ipv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
